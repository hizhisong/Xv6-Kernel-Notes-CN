# Lab: Mmap

Status: Completed
板块: Lab, page fault

开始Mmap实现前，先看看Mmap介绍与实操

[Mmap & Munmap](Mmap%20&%20Munmap%2048535b2bee114707a82b4752df35f448.md)

# 实现Mmap子集：file-backed shared & private mmap

## Xv6 Mmap Task：

**以Lazy的方式实现将file加载进内存中**

```c
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
```

- fd：基于文件 (file-backed)
- addr：假定为0，内核决定实际位置
- length：映射长度，不一定是文件大小
- prot：读写执行权限
- flags：file-backed下private或者shared映射
    - MAP_SHARED：对该内存区域的更新将被同步到文件中
    - MAP_PRIVATE：对该内存区域的更新**不会**被同步到文件中
    
    本实现中不要求多进程共享同一file、offset、MAP_SHARED的mmap到同一物理内存，仅需在同步写入磁盘这里做个要求
    
- offset：假定为0（从文件开头映射）
- 返回值：Success 内核分配内存地址 或者 Fail 0xffffffffffffffff

>>> tips: It's OK if processes that map the same MAP_SHARED file do **not** share physical pages.

```c
int munmap(void *addr, size_t length);
```

- 如果以MAP_SHARED形式被map则内部需要先将内存中内容刷入文件
- 部分区域munmap
    
    为例简便期间，题目中规定unmap的区域只可能是一个VMA范围里的{start, mid} or {mid, end} or {start, end}，因此不会导致munmap后映射区域被分为两块
    

## Solution:

1. 添加系统调用，注意这里在usys.pl中添加生成两个系统调用的系统调用号及ecall指令的脚本代码
2. 利用Trap处理Page Fault来Lazy的方式实现Mmap 
(The reason to be lazy is to ensure that mmap of a large file is fast, and that mmap of a file larger than physical memory is possible.)
3. 用户虚拟地址空间分配如下图：
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled.png)
    
    与Linux Mmap实现不太一样，在Xv6中我们固定选择一个用户虚拟地址空间Trapframe下的一部分为Mmap使用，和栈一样，自高地址向低地址增长，最多增长到proc()->sz处，proc()->sz是heap顶部的地址。（但mmap块内部使用是低地址向高地址增长）
    
    > **Tips**.
    用户进程在陷入内核态时执行内核函数的内核栈在procinit时会在内核虚拟地址空间分配，p->kernel_stack记录，不在用户虚拟地址空间实际划分，而是在进入Trap时uservec实现sp指针切换
    > 
4. 使用**每个进程**使用16个VMA结构体，每个VMA可以表示一次mmap的虚拟地址空间表示范围等元信息
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%201.png)
    
    跟踪mmap为每个流程映射的内容。定义VMA（虚拟内存区域）对应的结构，记录mmap创建的虚拟内存范围的地址、长度、权限、文件等。
    由于xv6内核中没有内存分配器，所以每个进程可以声明一个固定大小的VMA数组，并根据需要从该数组中进行分配。数组大小为16就足够了。
    
    ![struct proc](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%202.png)
    
    struct proc
    
    在每个进程PCB中添加VMA结构体数组，并且对下一个mmap可以使用的进程空闲空间进行记录，以便下次mmap时使用
    
5. 添加**mmap**系统调用的具体实现：
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%203.png)
    
    1. 首先对用户调用mmap系统调用时的参数合法性 和 文件读写与mmap读写一致性 进行检查
    2. 在内核中对该进程选择一个他自己空闲的VMA作为一片连续Mmap空间的元信息记录
        
        这里填入VMA中要mmap虚拟地址范围等信息，**并不实际分配物理内存、添加页表映射、读取disk文件**
        
    3. 增加**文件的引用计数**，以便在fd被关闭时mmap还能对对应文件实际操作 **<key>**
6. 在Trap中处理非法访问(未实际分配物理内存)物理地址 -- **实际Allocate物理内存**
    
    >>> 映射粒度就是每次一页，页表特性决定的
    
    一旦进程访问到刚刚mmap的空间，这些空间并没有实际分配物理内存与之对应，因此对该虚拟地址空间访问(load，store)将会触发Page Fault
    
    和Lazy Allocation类似，在trap中处理page fault，实际分配物理内存后再次执行出错指令访问mmap的内存。
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%204.png)
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%205.png)
    
    1. 在进程的VMA表中查找，访问出错的虚拟地址在哪一个VMA来mmap的地址范围
    2. 由于**内存分配的最小单位是一个Page**，因此在找到该VMA后，kalloc一个Page
    3. 记录下mmap读写权限将物理页映射到用户页表中
    4. 从disk中读取要映射的文件到这个Page中，当然注意读取文件offset
7. 此时访问刚刚未映射的mmap区域的指令就正常执行了
8. 添加**munmap**系统调用的具体实现：
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%206.png)
    
    1. 检查用户参数合法性
    2. 在进程的VMA表中查找，要取消映射的开始虚拟地址在哪一个VMA的地址范围里
    3. 对这个地址范围内的每一页检查，对已经映射物理内存的页才进行munmap处理
        1. 由于我们实现的mmap是基于文件的mmap
            
            因此在MAP_PRIVATE模式下取消页表映射并kfree掉页表映射的物理内存页
            
            在MAP_SHARED模式下映射取消页表映射物理内存，但不会释放对应的物理内存(仅当共享物理页的所有进程都取消映射时再kfree掉该物理页)
            
            实验没有要求，这里暂且没有实现 i. 的区分，一并给kfree掉
            
        2. 在MAP_PRIVATE模式下，mmap映射的物理内存**不将**其内存改动写入到disk文件中
            
            在MAP_SHARED模式下，mmap映射的物理内存**要将**其内存改动写入到disk文件中
            
        3. 取消映射指定范围后，我们要对mmap VMA里的映射范围元信息进行修改
        4. 如果我们发现该VMA原有范围内所有mmap映射地址空间全部都被munmap掉了，那我们就可以释放这个VMA再次可被利用
9. 在Xv6中，所有进程都是由第一个用户进程userinit进程开始一个一个fork出来的，我们在进程PCB中添加了一个`next_free_vmaddr`供进程在mmap时，在进程自己的虚拟地址空间中选择一个开始mmap的地址。
    
    因此我们就在第一个用户进程这里将其设置为mmap区域的开始地址，`TRAPFRAME`
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%207.png)
    
    接下来被fork出来的新进程，就可以继承上述MAP_SHARED的mmap区域，对于MAP_PRIAVTE或对其他未MAP_SHARED mmap过的文件，如果想mmap了就可以使用`p->next_free_vmaaddr`的区域防止和继承的MAP_SHARED的mmap区域重叠。
    
    当然本实验中没有加以区分MAP_SHARED与MAP_PRIAVTE mmap区域，所以在fork新子进程时，`p->next_free_vmaaddr` 、进程mmap VMAs一并复制下来，对于mmap的内容，理应仅对MAP_PRIAVTE的进行重新申请物理页并复制，但我们这里对于MAP_SHARED也一并复制下来，并没有作区分
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%208.png)
    
    ![复制已mmap区域](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%209.png)
    
    复制已mmap区域
    
    在一个进程退出以后，它应该munmap掉所有mmap的空间，我们在exit里处理：
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%2010.png)
    
    进程在执行exit时将自己状态设置成ZOMBIE，且最终调度，跳转至scheduler(sched())，不再返回，调度执行到该进程父进程时，父进程中调用的wait遍历PCB表，发现该ZOMBIE进程进行以下操作：
    
    1. 解除所有VMA使用
    2. 在父进程的wait中free该进程内存空间，包括mmap的物理内存
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%2011.png)
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%2012.png)
    
    ![Untitled](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%2013.png)
    
    ![对mmap后未实际分配物理内存的空间，我们要选择跳过，相较于uvmunmap函数](Lab%20Mmap%20ba596a8762a64b699764d27a9fb127f4/Untitled%2014.png)
    
    对mmap后未实际分配物理内存的空间，我们要选择跳过，相较于uvmunmap函数